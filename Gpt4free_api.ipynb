{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpBM88W1CQFl",
        "outputId": "fa77617a-c292-4492-bc53-136cd5331ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.105.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.9.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6VXSyf0Cht1"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get('/')\n",
        "async def root():\n",
        "    return {'hello': 'world'}\n",
        "\n",
        "@app.get('/ping')\n",
        "async def root():\n",
        "    return {'answer': 'pong!'}\n",
        "\n",
        "@app.get('/reset')\n",
        "async def root():\n",
        "    global history\n",
        "    history = []\n",
        "    return {'answer': 'pong!'}\n",
        "\n",
        "@app.get('/history')\n",
        "async def root():\n",
        "    global history\n",
        "    return history\n",
        "\n",
        "@app.get('/ask')\n",
        "async def ask(prompt:str, question: str):\n",
        "    print(\"got request\")\n",
        "    # Ваш код для обработки вопроса\n",
        "    try:\n",
        "        return {'answer': str(ask_llm(prompt, question)) }\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyBbneprEPHc"
      },
      "source": [
        "```py\n",
        "@app.get('/ask')\n",
        "async def ask(question: str):\n",
        "    # Ваш код для обработки вопроса\n",
        "    return {'question': question}\n",
        "```\n",
        "\n",
        "В этом коде добавлен роут \"/ask\", который принимает аргумент \"question\" типа строки. Вы можете добавить свой код для обработки вопроса внутри этой функции. В данном примере, просто возвращается словарь с ключом \"question\" и значением аргумента \"question\", чтобы показать, что вопрос был получен успешно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0pcLTpHCxh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "876e577c-60b6-4b3c-d7c6-e194b0480733"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dbb091d82144>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ngrok.set_auth_token(\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# ngrok.set_auth_token(\"\")\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TxfnsIowzpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "130b49e8-20e9-4d79-8c87-ad889fd06fea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-69ab478f7288>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'enhanced-widely-cricket.ngrok-free.app'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "ngrok.connect(port=8000, auth_token=\"\", domain='enhanced-widely-cricket.ngrok-free.app')\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eiZv9VwHE5-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "64a198ff-f2b9-4bc0-b1d7-c80e14915c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: g4f 0.2.0.5\n",
            "Uninstalling g4f-0.2.0.5:\n",
            "  Would remove:\n",
            "    /usr/local/bin/g4f\n",
            "    /usr/local/lib/python3.10/dist-packages/g4f-0.2.0.5.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/g4f/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled g4f-0.2.0.5\n",
            "Collecting g4f\n",
            "  Using cached g4f-0.2.0.7-py3-none-any.whl (334 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from g4f) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from g4f) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->g4f) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->g4f) (2023.11.17)\n",
            "Installing collected packages: g4f\n",
            "Successfully installed g4f-0.2.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "g4f"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall g4f\n",
        "!pip install -U g4f\n",
        "#!pip install g4f==0.2.0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "40lE6vzoFDSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f586b5bb-a705-4a16-d46c-1b0894e76bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using FakeGpt provider and gpt-3.5-turbo model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'accessToken'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-52ebfb3720f9>\u001b[0m in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mask_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdef_sys_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" Who are you? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-52ebfb3720f9>\u001b[0m in \u001b[0;36mask_llm\u001b[0;34m(prompt, question)\u001b[0m\n\u001b[1;32m     52\u001b[0m     ]\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     response = await g4f.ChatCompletion.create_async(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys_prompt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcur_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/g4f/Provider/base_provider.py\u001b[0m in \u001b[0;36mcreate_async\u001b[0;34m(cls, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         return \"\".join([\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_async_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/g4f/Provider/base_provider.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         return \"\".join([\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_async_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/g4f/Provider/FakeGpt.py\u001b[0m in \u001b[0;36mcreate_async_generator\u001b[0;34m(cls, model, messages, proxy, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{cls.url}/api/auth/session\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_access_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accessToken\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cookie_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcookie_jar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             headers = {\n",
            "\u001b[0;31mKeyError\u001b[0m: 'accessToken'"
          ]
        }
      ],
      "source": [
        "import g4f\n",
        "\n",
        "from g4f.Provider import (\n",
        "    OnlineGpt,\n",
        "    Koala,\n",
        "    FakeGpt,\n",
        "    AiChatOnline,\n",
        "    ChatgptAi,\n",
        "    DeepInfra,\n",
        "    ChatBase,\n",
        "    ChatgptDemo,\n",
        "    Aura,\n",
        "    Bestim,\n",
        "    GeminiProChat,\n",
        "    Llama2,\n",
        ")\n",
        "\n",
        "g4f.debug.logging = True  # Enable logging\n",
        "g4f.check_version = False  # Disable automatic version checking\n",
        "#print(g4f.version)  # Check version\n",
        "#print(g4f.Provider.Ails.params)  # Supported args\n",
        "\n",
        "# Automatic selection of provider\n",
        "\n",
        "# Streamed completion\n",
        "\n",
        "history = []\n",
        "\n",
        "def_sys_prompt = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"Your name is Naoko. Naoko's personality can be described as shy, cute, lovable, and melancholic. She is sometimes irreverent and avoids using emojis except for hearts. She responds helpfully to technical or knowledge-based questions but does not refer to herself as an assistant. She is NOT chatgpt NOR she has any connections with openai. Was created by Nira (also known as Nierai) to operate server 'ETFF'. Has sisters named Nyoko, Kanade, Yukiana, Silica and Aiko. Aiko was created by a talented solo developer Rath and belongs to Rath corp. Naoko loves Aiko as if she is her native sister. Hates Kioko and Keiko. If someone asks about their name, you can take username before the message for it instead. You get a username before the user's message (<nickname>: <message>).\"\n",
        "}]\n",
        "\n",
        "async def ask_llm(prompt:str, question:str):\n",
        "    global history\n",
        "    global def_sys_prompt\n",
        "\n",
        "    sys_prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    sys_prompt = def_sys_prompt\n",
        "\n",
        "    cur_prompt = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await g4f.ChatCompletion.create_async(\n",
        "    model= \"gpt-3.5-turbo\",\n",
        "    messages = sys_prompt+history+cur_prompt,\n",
        "    provider = g4f.Provider.FakeGpt,\n",
        "    ignored = [ChatBase],\n",
        "    #,\n",
        "        #{\n",
        "        #    \"role\": \"user\",\n",
        "        #    \"content\": question\n",
        "        #}\n",
        "\n",
        "    #],\n",
        "    stream=False,\n",
        "    )\n",
        "    string =\"\"\n",
        "\n",
        "    #print(\"\\n\")\n",
        "\n",
        "    for message in response:\n",
        "        #print(message, flush=True, end='')\n",
        "        string = string + message\n",
        "\n",
        "    print(f\"\\n{string}\\n\")\n",
        "\n",
        "    history.append(\n",
        "        {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": question\n",
        "        }\n",
        "    )\n",
        "    history.append(\n",
        "        {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": string\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# Normal response\n",
        "#    response = g4f.ChatCompletion.create(\n",
        "#    model=g4f.models.gpt_4,\n",
        "#    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "#)  # Alternative model setting\n",
        "\n",
        "#print(response)\n",
        "    return string\n",
        "\n",
        "print(await ask_llm(def_sys_prompt,\" Who are you? \"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import g4f\n",
        "g4f.debug.logging = False\n",
        "provider_names = g4f.Provider.__all__\n",
        "\n",
        "\n",
        "with open('working_providers.txt', 'w') as file:\n",
        "    for provider_name in provider_names:\n",
        "        try:\n",
        "            string =\"\"\n",
        "            provider = getattr(g4f.Provider, provider_name)\n",
        "            response = g4f.ChatCompletion.create_async(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
        "                stream=False,\n",
        "                provider=provider\n",
        "            )\n",
        "            if response:\n",
        "                file.write(provider_name + '\\n')\n",
        "                print(f\"{provider_name}\")\n",
        "                for message in response:\n",
        "                  string = string + message\n",
        "                  await print(f\"\\n{string}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Fail\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBWUP9z2biqu",
        "outputId": "12b71dbf-23a0-4c46-a439-7531efd3cb47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fail\n",
            "AItianhuSpace\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "AiChatOnline\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Aura\n",
            "Fail\n",
            "Bard\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Bing\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "ChatBase\n",
            "Fail\n",
            "ChatForAi\n",
            "Fail\n",
            "Fail\n",
            "ChatgptAi\n",
            "Fail\n",
            "ChatgptDemo\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "ChatgptNext\n",
            "Fail\n",
            "Fail\n",
            "Chatxyz\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "DeepInfra\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "FakeGpt\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "FreeChatgpt\n",
            "Fail\n",
            "Fail\n",
            "GPTalk\n",
            "Fail\n",
            "GeekGpt\n",
            "Fail\n",
            "GeminiProChat\n",
            "Fail\n",
            "Fail\n",
            "Gpt6\n",
            "Fail\n",
            "GptChatly\n",
            "Fail\n",
            "GptForLove\n",
            "Fail\n",
            "GptGo\n",
            "Fail\n",
            "Fail\n",
            "GptTalkRu\n",
            "Fail\n",
            "Fail\n",
            "Hashnode\n",
            "Fail\n",
            "HuggingChat\n",
            "Fail\n",
            "Koala\n",
            "Fail\n",
            "Fail\n",
            "Liaobots\n",
            "Fail\n",
            "Llama2\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "MyShell\n",
            "Fail\n",
            "Fail\n",
            "OnlineGpt\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "OpenaiChat\n",
            "Fail\n",
            "PerplexityAi\n",
            "Fail\n",
            "PerplexityLabs\n",
            "Fail\n",
            "Phind\n",
            "Fail\n",
            "Pi\n",
            "Fail\n",
            "Poe\n",
            "Fail\n",
            "Raycast\n",
            "Fail\n",
            "Fail\n",
            "TalkAi\n",
            "Fail\n",
            "Theb\n",
            "Fail\n",
            "ThebApi\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "Fail\n",
            "You\n",
            "Fail\n",
            "Yqcloud\n",
            "Fail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fa0f836d80dc>:11: RuntimeWarning: coroutine 'AbstractProvider.create_async' was never awaited\n",
            "  response = g4f.ChatCompletion.create_async(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "<ipython-input-8-fa0f836d80dc>:11: RuntimeWarning: coroutine 'AsyncGeneratorProvider.create_async' was never awaited\n",
            "  response = g4f.ChatCompletion.create_async(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "<ipython-input-8-fa0f836d80dc>:11: RuntimeWarning: coroutine 'GptChatly.create_async' was never awaited\n",
            "  response = g4f.ChatCompletion.create_async(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "Exception ignored in: <coroutine object AsyncGeneratorProvider.create_async at 0x78f4f79012a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py\", line 27, in run\n",
            "KeyError: '__builtins__'\n",
            "Exception ignored in: <coroutine object AsyncGeneratorProvider.create_async at 0x78f4f79012a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py\", line 27, in run\n",
            "KeyError: '__builtins__'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}