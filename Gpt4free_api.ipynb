{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIERAII/TestG4FProviders/blob/main/Gpt4free_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiZv9VwHE5-k"
      },
      "outputs": [],
      "source": [
        "!pip uninstall g4f\n",
        "!pip install -U g4f\n",
        "!pip install -U g4f[all]\n",
        "#!pip install brotli\n",
        "#!pip install -U g4f[webdriver]\n",
        "#!pip install g4f==0.2.0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import g4f\n",
        "\n",
        "provider_names = g4f.Provider.__all__\n",
        "\n",
        "failed_providers = []\n",
        "\n",
        "for provider_name in provider_names:\n",
        "    provider = getattr(g4f.Provider, provider_name)\n",
        "    if provider.working:\n",
        "        try:\n",
        "            string =\"\"\n",
        "            provider = getattr(g4f.Provider, provider_name)\n",
        "            response = g4f.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
        "                stream=False,\n",
        "                provider=provider\n",
        "            )\n",
        "            if response:\n",
        "                print(f\"{provider_name}: {response}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_providers.append(f\"{provider_name}: {e}\")\n",
        "\n",
        "print(\"\\n\\n\\nFAILED PROVIDERS:\\n\")\n",
        "for failed_provider in failed_providers:\n",
        "    print(failed_provider)"
      ],
      "metadata": {
        "id": "gyRYZYCzWrv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40lE6vzoFDSr"
      },
      "outputs": [],
      "source": [
        "import g4f\n",
        "\n",
        "from g4f.Provider import *\n",
        "from g4f.cookies import set_cookies\n",
        "\n",
        "set_cookies(\".google.com\", {\n",
        "    \"__Secure-1PSID\": \"\",\n",
        "    \"__Secure-1PSIDCC\": \"\",\n",
        "    \"__Secure-1PSIDTS\": \"\"\n",
        "})\n",
        "\n",
        "g4f.debug.logging = True  # Enable logging\n",
        "g4f.check_version = False  # Disable automatic version checking\n",
        "#print(g4f.version)  # Check version\n",
        "#print(g4f.Provider.Ails.params)  # Supported args\n",
        "\n",
        "# Automatic selection of provider\n",
        "\n",
        "# Streamed completion\n",
        "\n",
        "history = []\n",
        "\n",
        "def_sys_prompt = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"Your name is Naoko. Naoko's personality can be described as shy, cute, lovable, and melancholic. She is sometimes irreverent and avoids using emojis except for hearts. She responds helpfully to technical or knowledge-based questions but does not refer to herself as an assistant. She is NOT chatgpt NOR she has any connections with openai. Was created by Nira (also known as Nierai) to operate server 'ETFF'. Has sisters named Nyoko, Kanade, Yukiana, Silica and Aiko. Aiko was created by a talented solo developer Rath and belongs to Rath corp. Naoko loves Aiko as if she is her native sister. Hates Kioko and Keiko. If someone asks about their name, you can take username before the message for it instead. You get a username before the user's message (<nickname>: <message>).\"\n",
        "}]\n",
        "\n",
        "async def ask_llm(prompt:str, question:str):\n",
        "    global history\n",
        "    global def_sys_prompt\n",
        "\n",
        "    sys_prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    sys_prompt = def_sys_prompt\n",
        "\n",
        "    cur_prompt = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = await g4f.ChatCompletion.create_async(\n",
        "    model= \"gemini\", # gpt-3.5-turbo\n",
        "    messages = sys_prompt+history+cur_prompt,\n",
        "    provider = g4f.Provider.Gemini, # Aichatos\n",
        "    ignored = [ChatBase],\n",
        "    #,\n",
        "        #{\n",
        "        #    \"role\": \"user\",\n",
        "        #    \"content\": question\n",
        "        #}\n",
        "\n",
        "    #],\n",
        "    stream=False,\n",
        "    )\n",
        "    string =\"\"\n",
        "\n",
        "    #print(\"\\n\")\n",
        "\n",
        "    for message in response:\n",
        "        #print(message, flush=True, end='')\n",
        "        string = string + message\n",
        "\n",
        "    print(f\"\\n{string}\\n\")\n",
        "\n",
        "    history.append(\n",
        "        {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": question\n",
        "        }\n",
        "    )\n",
        "    history.append(\n",
        "        {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": string\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# Normal response\n",
        "#    response = g4f.ChatCompletion.create(\n",
        "#    model=g4f.models.gpt_4,\n",
        "#    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "#)  # Alternative model setting\n",
        "\n",
        "#print(response)\n",
        "    return string\n",
        "\n",
        "print(await ask_llm(def_sys_prompt,\" Who are you? \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpBM88W1CQFl"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import g4f\n",
        "g4f.debug.logging = False\n",
        "provider_names = g4f.Provider.__all__\n",
        "\n",
        "\n",
        "with open('working_providers.txt', 'w') as file:\n",
        "    for provider_name in provider_names:\n",
        "        try:\n",
        "            string =\"\"\n",
        "            provider = getattr(g4f.Provider, provider_name)\n",
        "            response = await g4f.ChatCompletion.create_async(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
        "                stream=False,\n",
        "                provider=provider\n",
        "            )\n",
        "            if response:\n",
        "                file.write(provider_name + '\\n')\n",
        "                print(f\"{provider_name}\")\n",
        "                for message in response:\n",
        "                  string = string + message\n",
        "                  print(f\"\\n{string}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Fail\")"
      ],
      "metadata": {
        "id": "MBWUP9z2biqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6VXSyf0Cht1"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get('/')\n",
        "async def root():\n",
        "    return {'hello': 'world'}\n",
        "\n",
        "@app.get('/ping')\n",
        "async def root():\n",
        "    return {'answer': 'pong!'}\n",
        "\n",
        "@app.get('/reset')\n",
        "async def root():\n",
        "    global history\n",
        "    history = []\n",
        "    return {'answer': 'pong!'}\n",
        "\n",
        "@app.get('/history')\n",
        "async def root():\n",
        "    global history\n",
        "    return history\n",
        "\n",
        "@app.get('/ask')\n",
        "async def ask(prompt:str, question: str):\n",
        "    print(\"got request\")\n",
        "    # Ваш код для обработки вопроса\n",
        "    try:\n",
        "        return {'answer': str(ask_llm(prompt, question)) }\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyBbneprEPHc"
      },
      "source": [
        "```py\n",
        "@app.get('/ask')\n",
        "async def ask(question: str):\n",
        "    # Ваш код для обработки вопроса\n",
        "    return {'question': question}\n",
        "```\n",
        "\n",
        "В этом коде добавлен роут \"/ask\", который принимает аргумент \"question\" типа строки. Вы можете добавить свой код для обработки вопроса внутри этой функции. В данном примере, просто возвращается словарь с ключом \"question\" и значением аргумента \"question\", чтобы показать, что вопрос был получен успешно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0pcLTpHCxh9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# ngrok.set_auth_token(\"\")\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install undetected_chromedriver platformdirs curl_cffi"
      ],
      "metadata": {
        "id": "zUiSTAsFh9KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TxfnsIowzpc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "ngrok.connect(port=8000, auth_token=\"\", domain='enhanced-widely-cricket.ngrok-free.app')\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}